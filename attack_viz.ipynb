{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named '__main__.attack_utils'; '__main__' is not a package",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-9ad41d5b9620>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mload_dataset_custom\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0minit_dirs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_naming\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_file_save_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_probs_save_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mattack_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcal_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgenerate_target_label_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpgd_attack\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpgd_l2_attack\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhybrid_attack\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mdata_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload_dataset_tensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named '__main__.attack_utils'; '__main__' is not a package"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['CUDA_DEVICE_ORDER'] = 'PCI_BUS_ID'\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1'\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd.gradcheck import zero_gradients\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import time\n",
    "import argparse\n",
    "from torchsummary import summary\n",
    "import torchvision\n",
    "from scipy.special import softmax\n",
    "import collections\n",
    "import json \n",
    "\n",
    "from utils.mnist_models import cnn_3l_bn\n",
    "from utils.resnet_cifar import resnet\n",
    "from utils.test_utils import test, robust_test, robust_test_hybrid\n",
    "from utils.data_utils import load_dataset, load_dataset_custom\n",
    "from utils.io_utils import init_dirs, model_naming, test_file_save_name, test_probs_save_name\n",
    "from utils.attack_utils import cal_loss, generate_target_label_tensor, pgd_attack, pgd_l2_attack, hybrid_attack\n",
    "from utils.data_utils import load_dataset_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def robust_test(model, loss_fn, loader, args, att_dir, epoch=0, training_output_dir_name=None, \n",
    "                figure_dir_name=None, n_batches=0, train_data=False, \n",
    "                training_time=False):\n",
    "    \"\"\"\n",
    "    n_batches (int): Number of batches for evaluation.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    num_correct, num_correct_adv, num_samples = 0, 0, 0\n",
    "    steps = 1\n",
    "    losses_adv = []\n",
    "    losses_ben = []\n",
    "    prob_dict = {}\n",
    "    adv_examples = []\n",
    "    adv_preds = []\n",
    "    true_labels = []\n",
    "\n",
    "    for t, (x, y, idx, ez, m) in enumerate(loader):\n",
    "        x = x.cuda()\n",
    "        y = y.cuda()\n",
    "        x_var = Variable(x, requires_grad= True)\n",
    "        y_var = Variable(y, requires_grad=False)\n",
    "        if att_dir['targeted']:\n",
    "            y_target = generate_target_label_tensor(\n",
    "                               y_var.cpu(), args).cuda()\n",
    "        else:\n",
    "            y_target = y_var\n",
    "        if 'PGD_linf' in att_dir['attack']:\n",
    "            adv_x = pgd_attack(model, x, x_var, y_target, att_dir['attack_iter'],\n",
    "                           att_dir['epsilon'], att_dir['eps_step'], att_dir['clip_min'],\n",
    "                           att_dir['clip_max'], att_dir['targeted'], att_dir['rand_init'])\n",
    "        elif 'PGD_l2' in att_dir['attack']:\n",
    "            adv_x = pgd_l2_attack(model, x, x_var, y_target, att_dir['attack_iter'],\n",
    "                           att_dir['epsilon'], att_dir['eps_step'], att_dir['clip_min'],\n",
    "                           att_dir['clip_max'], att_dir['targeted'], att_dir['rand_init'], \n",
    "                           att_dir['num_restarts'])\n",
    "        adv_examples.append(adv_x)\n",
    "        # Predictions\n",
    "        scores = model(x.cuda()) \n",
    "        _, preds = scores.data.max(1)\n",
    "        scores_adv = model(adv_x)\n",
    "        _, preds_adv = scores_adv.data.max(1)\n",
    "        adv_preds.append(preds_adv)\n",
    "        true_labels.append(y)\n",
    "        # Losses\n",
    "        batch_loss_adv = loss_fn(scores_adv, y)\n",
    "        loss_adv = torch.mean(batch_loss_adv)\n",
    "        losses_adv.append(loss_adv.data.cpu().numpy())\n",
    "        batch_loss_ben = loss_fn(scores, y)\n",
    "        loss_ben = torch.mean(batch_loss_ben)\n",
    "        losses_ben.append(loss_ben.data.cpu().numpy())\n",
    "        # Correct count\n",
    "        num_correct += (preds == y).sum()\n",
    "        num_correct_adv += (preds_adv == y).sum()\n",
    "        num_samples += len(preds)\n",
    "        \n",
    "        if not training_time:\n",
    "          if args.viz and steps == 1:\n",
    "              if not os.path.exists(figure_dir_name):\n",
    "                os.makedirs(figure_dir_name)\n",
    "              custom_save_image(adv_x, preds_adv, y, args, figure_dir_name, \n",
    "                                train_data)\n",
    "        \n",
    "        if n_batches > 0 and steps==n_batches:\n",
    "            break\n",
    "        steps += 1\n",
    "\n",
    "    acc = float(num_correct) / num_samples\n",
    "    acc_adv = float(num_correct_adv) / num_samples\n",
    "    print('Clean accuracy: {:.2f}% ({}/{})'.format(\n",
    "        100.*acc,\n",
    "        num_correct,\n",
    "        num_samples,\n",
    "    ))\n",
    "    print('Adversarial accuracy: {:.2f}% ({}/{})'.format(\n",
    "        100.*acc_adv,\n",
    "        num_correct_adv,\n",
    "        num_samples,\n",
    "    ))\n",
    "\n",
    "    return 100.*acc, 100.*acc_adv, np.mean(losses_ben), np.mean(losses_adv), adv_examples, adv_preds, true_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.random.manual_seed(7)\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "# Data args\n",
    "parser.add_argument('--dataset_in', type=str, default='MNIST')\n",
    "parser.add_argument('--n_classes', type=int, default=10)\n",
    "parser.add_argument('--num_samples', type=int, default=None)\n",
    "\n",
    "# Model args\n",
    "parser.add_argument('--model', type=str, default='cnn_3l',\n",
    "                    choices=['resnet','wrn', 'cnn_3l', 'cnn_3l_bn', 'dn'])\n",
    "parser.add_argument('--conv_expand', type=int, default=1)\n",
    "parser.add_argument('--fc_expand', type=int, default=1)\n",
    "parser.add_argument('--depth', type=int, default=28)\n",
    "parser.add_argument('--width', type=int, default=10)\n",
    "parser.add_argument('--lr_schedule', type=str, default='linear0')\n",
    "parser.add_argument('--batch_size', type=int, default=128)\n",
    "parser.add_argument('--test_batch_size', type=int, default=128)\n",
    "parser.add_argument('--learning_rate', type=float, default=0.1)\n",
    "# parser.add_argument('--weight_decay', type=float, default=2e-4)\n",
    "\n",
    "# Defense args\n",
    "parser.add_argument('--is_adv', dest='is_adv', action='store_true')\n",
    "parser.add_argument('--attack', type=str, default='PGD_l2',\n",
    "                    choices=['PGD_l2', 'PGD_linf', 'PGD_l2_hybrid_seed', 'PGD_l2_hybrid_replace'])\n",
    "parser.add_argument('--epsilon', type=float, default=8.0)\n",
    "parser.add_argument('--attack_iter', type=int, default=10)\n",
    "parser.add_argument('--gamma', type=float, default=1.0)\n",
    "parser.add_argument('--eps_step', type=float, default=2.0)\n",
    "parser.add_argument('--is_dropping', dest='dropping', action='store_true')\n",
    "parser.add_argument('--rand_init', dest='rand_init', action='store_true')\n",
    "parser.add_argument('--eps_schedule', type=int, default=0)\n",
    "parser.add_argument('--num_restarts', type=int, default=1)\n",
    "parser.add_argument('--marking_strat', type=str, default=None)\n",
    "parser.add_argument('--matching_path', type=str, default='matchings')\n",
    "parser.add_argument('--degree_path', type=str, default='graph_data/degree_results')\n",
    "parser.add_argument(\"--norm\", default='l2', help=\"norm to be used\")\n",
    "parser.add_argument('--drop_thresh', type=int, default=100)\n",
    "parser.add_argument('--drop_eps',type=float, default=0.0)\n",
    "parser.add_argument('--curriculum', type=str, default='all')\n",
    "parser.add_argument('--loss_fn', type=str, default='CE')\n",
    "\n",
    "# Attack args\n",
    "parser.add_argument('--new_attack', type=str, default='PGD_l2',\n",
    "                    choices=['PGD_l2', 'PGD_linf', 'PGD_l2_hybrid_seed', 'PGD_l2_hybrid_replace'])\n",
    "parser.add_argument('--new_epsilon', type=float, default=2.0)\n",
    "parser.add_argument('--new_attack_iter', type=int, default=20)\n",
    "parser.add_argument('--new_gamma', type=float, default=1.0)\n",
    "parser.add_argument('--targeted', dest='targeted', action='store_true')\n",
    "parser.add_argument('--clip_min', type=float, default=0)\n",
    "parser.add_argument('--clip_max', type=float, default=1.0)\n",
    "parser.add_argument('--new_rand_init',\n",
    "                    dest='new_rand_init', action='store_true')\n",
    "parser.add_argument('--new_num_restarts', type=int, default=1)\n",
    "parser.add_argument('--new_marking_strat', type=str, default=None)\n",
    "\n",
    "# IO args\n",
    "parser.add_argument('--last_epoch', type=int, default=0)\n",
    "parser.add_argument('--checkpoint_path', type=str, \n",
    "                    default='/data/abhagoji/models')\n",
    "parser.add_argument('--is_viz', dest='viz', action='store_true')\n",
    "\n",
    "# Trial args\n",
    "parser.add_argument('--num_of_trials', type=int, default=1)\n",
    "parser.add_argument('--save_test', dest='save_test', action='store_true')\n",
    "parser.add_argument('--track_hard', dest='track_hard', action='store_true')\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print('CUDA enabled')\n",
    "else:\n",
    "    raise ValueError('Needs a working GPU!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps=3.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = parser.parse_args(\"--dataset=MNIST --n_classes=2 --num_samples=5000 --model=cnn_3l_bn --attack=PGD_l2 --epsilon={} --attack_iter=50 --gamma=2.5 --new_attack=PGD_l2 --new_epsilon={} --new_attack_iter=50 --new_gamma=2.5 --trial_num=1 --is_adv  --rand_init --new_rand_init\".format(eps,eps).split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = parser.parse_args()\n",
    "\n",
    "if args.num_samples is None:\n",
    "    args.num_samples = 'All'\n",
    "if args.drop_eps==0:\n",
    "    args.drop_eps=args.epsilon\n",
    "\n",
    "args.checkpoint_path = 'trained_models'\n",
    "\n",
    "args.eps_step = args.epsilon*args.gamma/args.attack_iter\n",
    "args.new_eps_step = args.new_epsilon*args.gamma/args.new_attack_iter\n",
    "attack_params = {'attack': args.new_attack, 'epsilon': args.new_epsilon, \n",
    "             'attack_iter': args.new_attack_iter, 'eps_step': args.new_eps_step,\n",
    "             'targeted': args.targeted, 'clip_min': args.clip_min,\n",
    "             'clip_max': args.clip_max,'rand_init': args.new_rand_init, \n",
    "             'num_restarts': args.new_num_restarts}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.trial_num = 1\n",
    "model_dir_name, log_dir_name, figure_dir_name, _ = init_dirs(\n",
    "    args, train=False)\n",
    "_, model_name = model_naming(args)\n",
    "print('Loading %s' % model_dir_name)\n",
    "\n",
    "training_time = False\n",
    "\n",
    "if args.n_classes != 10:\n",
    "    loader_train, loader_test, data_details = load_dataset_custom(\n",
    "        args, data_dir='data', training_time=training_time)\n",
    "else:\n",
    "    loader_train, loader_test, data_details = load_dataset(\n",
    "        args, data_dir='data', training_time=training_time)\n",
    "\n",
    "num_channels = data_details['n_channels']\n",
    "\n",
    "if 'MNIST' in args.dataset_in:\n",
    "    if 'cnn_3l_bn' in args.model:\n",
    "        net = cnn_3l_bn(args.n_classes, args.conv_expand, args.fc_expand)\n",
    "\n",
    "if 'linf' in args.attack:\n",
    "    args.epsilon /= 255.\n",
    "    args.eps_step /= 255.\n",
    "\n",
    "print(\"Using batch size of {}\".format(args.batch_size))\n",
    "\n",
    "net.cuda()\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(reduction='none')\n",
    "\n",
    "net.eval()\n",
    "ckpt_path = 'checkpoint_' + str(args.last_epoch)\n",
    "net.load_state_dict(torch.load(model_dir_name + ckpt_path))\n",
    "if 'hybrid' in args.new_attack:\n",
    "    print('Using attack {}'.format(args.new_attack))\n",
    "    f_eval = robust_test_hybrid\n",
    "else:\n",
    "    print('Using attack {}'.format(args.new_attack))\n",
    "    f_eval = robust_test\n",
    "n_batches_eval = 0\n",
    "print('Training data results')\n",
    "acc_train, acc_train_adv, loss_train, loss_train_adv, adv_examples, adv_preds, true_labels = f_eval(net, \n",
    "    criterion, loader_train, args, attack_params, 0, None, \n",
    "    figure_dir_name, n_batches=n_batches_eval, train_data=True, training_time=training_time)\n",
    "print('Test data results')\n",
    "acc_test, acc_test_adv, loss_test, loss_test_adv, adv_examples, adv_preds, true_labels = f_eval(net, \n",
    "    criterion, loader_test, args, attack_params, 0, None, \n",
    "    figure_dir_name, n_batches=n_batches_eval, train_data=False, training_time=training_time)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
